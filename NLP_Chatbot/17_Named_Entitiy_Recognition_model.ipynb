{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "# Named Entity Recognition (NER) is a natural language processing task that involves \n",
    "# identifying and classifying named entities \n",
    "# (such as person names, locations, organizations, dates, etc.) in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 파일 불러오기\n",
    "def read_file(file_name):\n",
    "    sents = []\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, l in enumerate(lines):\n",
    "            if l[0] == ';' and lines[idx + 1][0] == '$':\n",
    "                this_sent = []\n",
    "            elif l[0] == '$' and lines[idx - 1][0] == ';':\n",
    "                continue\n",
    "            elif l[0] == '\\n':\n",
    "                sents.append(this_sent)\n",
    "            else:\n",
    "                this_sent.append(tuple(l.split()))\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 크기 : \n",
      " 3555\n",
      "0번째 샘플 문장 시퀀스 : \n",
      " ['한편', ',', 'AFC', '챔피언스', '리그', 'E', '조', '에', '속하', 'ㄴ', '포항', '역시', '대회', '8강', '진출', '이', '불투명', '하', '다', '.']\n",
      "0번째 샘플 bio 태그 : \n",
      " ['O', 'O', 'O', 'O', 'O', 'B_OG', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "샘플 문장 시퀀스 최대 길이 : 168\n",
      "샘플 문장 시퀀스 평균 길이 : 34.03909985935302\n"
     ]
    }
   ],
   "source": [
    "# 학습용 말뭉치 데이터를 불러옴\n",
    "corpus = read_file('train.txt')\n",
    "\n",
    "# 말뭉치 데이터에서 단어와 BIO 태그만 불러와 학습용 데이터셋 생성\n",
    "sentences, tags = [], []\n",
    "for t in corpus:\n",
    "    tagged_sentence = []\n",
    "    sentence, bio_tag = [], []\n",
    "    for w in t:\n",
    "        tagged_sentence.append((w[1], w[3]))\n",
    "        sentence.append(w[1])\n",
    "        bio_tag.append(w[3])\n",
    "\n",
    "    sentences.append(sentence)\n",
    "    tags.append(bio_tag)\n",
    "\n",
    "print(\"샘플 크기 : \\n\", len(sentences))\n",
    "print(\"0번째 샘플 문장 시퀀스 : \\n\", sentences[0])\n",
    "print(\"0번째 샘플 bio 태그 : \\n\", tags[0])\n",
    "print(\"샘플 문장 시퀀스 최대 길이 :\", max(len(l) for l in sentences))\n",
    "print(\"샘플 문장 시퀀스 평균 길이 :\", (sum(map(len, sentences))/len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', '한편', 'NNG', 'O'), ('1', ',', 'SP', 'O'), ('2', 'AFC', 'SL', 'O'), ('2', '챔피언스', 'NNG', 'O'), ('2', '리그', 'NNG', 'O'), ('3', 'E', 'SL', 'B_OG'), ('3', '조', 'NNG', 'I'), ('3', '에', 'JKB', 'O'), ('4', '속하', 'VV', 'O'), ('4', 'ㄴ', 'ETM', 'O'), ('5', '포항', 'NNP', 'O'), ('6', '역시', 'MAJ', 'O'), ('7', '대회', 'NNG', 'O'), ('8', '8강', 'NNG', 'O'), ('9', '진출', 'NNG', 'O'), ('9', '이', 'JKS', 'O'), ('10', '불투명', 'NNG', 'O'), ('10', '하', 'VV', 'O'), ('10', '다', 'EC', 'O'), ('11', '.', 'SF', 'O')]\n",
      "[('1', '2003', 'SN', 'B_DT'), ('1', '년', 'NNB', 'I'), ('2', '6', 'SN', 'I'), ('2', '월', 'NNB', 'I'), ('3', '14', 'SN', 'I'), ('3', '일', 'NNB', 'I'), ('4', '사직', 'NNG', 'O'), ('5', '두산', 'NNP', 'O'), ('5', '전', 'NNG', 'O'), ('6', '이후', 'NNG', 'O'), ('7', '박명환', 'NNP', 'B_PS'), ('7', '에게', 'JKB', 'O'), ('8', '당하', 'VV', 'O'), ('8', '았', 'EP', 'O'), ('8', '던', 'ETM', 'O'), ('9', '10', 'SN', 'O'), ('9', '연패', 'NNG', 'O'), ('10', '사슬', 'NNG', 'O'), ('10', '을', 'JKO', 'O'), ('11', '거의', 'MAG', 'O'), ('12', '5', 'SN', 'B_DT'), ('12', '년', 'NNB', 'I'), ('13', '만', 'NNB', 'O'), ('13', '에', 'JKB', 'O'), ('14', '끊', 'VV', 'O'), ('14', '는', 'ETM', 'O'), ('15', '의미', 'NNG', 'O'), ('15', '있', 'VV', 'O'), ('15', '는', 'ETM', 'O'), ('16', '승리', 'NNG', 'O'), ('16', '이', 'VCP', 'O'), ('16', '었', 'EP', 'O'), ('16', '다', 'EC', 'O'), ('17', '.', 'SF', 'O')]\n"
     ]
    }
   ],
   "source": [
    "corpus = read_file('train.txt')\n",
    "print(corpus[0])\n",
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['한편', ',', 'AFC', '챔피언스', '리그', 'E', '조', '에', '속하', 'ㄴ', '포항', '역시', '대회', '8강', '진출', '이', '불투명', '하', '다', '.'], ['2003', '년', '6', '월', '14', '일', '사직', '두산', '전', '이후', '박명환', '에게', '당하', '았', '던', '10', '연패', '사슬', '을', '거의', '5', '년', '만', '에', '끊', '는', '의미', '있', '는', '승리', '이', '었', '다', '.'], ['AP', '통신', '은', '8', '일', '(', '이하', '한국', '시간', ')', '올라주원', ',', '유잉', '을', '비롯', '하', '아', '애드리언', '댄틀리', ',', '팻', '라일리', '감독', ',', '캐시', '러시', '감독', ',', 'TV', '해설가', '딕', '바이텔', ',', '디트로이트', '피스톤스', '의', '구단주', '윌리엄', '데이비드슨', '등', '이', '2008', '명예', '의', '전당', '헌액', '자', '로', '결정', '되', '었', '다고', '보', '아도', '하', '았', '다', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O', 'O', 'O', 'O', 'O', 'B_OG', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B_DT', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B_PS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_DT', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B_OG', 'I', 'O', 'B_DT', 'I', 'O', 'O', 'B_LC', 'O', 'O', 'B_PS', 'O', 'B_PS', 'O', 'O', 'O', 'O', 'B_PS', 'I', 'O', 'B_PS', 'I', 'O', 'O', 'B_PS', 'I', 'O', 'O', 'O', 'O', 'B_PS', 'I', 'O', 'B_OG', 'I', 'O', 'O', 'B_PS', 'I', 'O', 'O', 'B_DT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(tags[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 정의\n",
    "sent_tokenizer = preprocessing.text.Tokenizer(oov_token='OOV') # 첫 번째 인덱스에는 OOV 사용\n",
    "sent_tokenizer.fit_on_texts(sentences)\n",
    "tag_tokenizer = preprocessing.text.Tokenizer(lower=False) # 태그 정보는 lower= False 소문자로 변환하지 않는다.\n",
    "tag_tokenizer.fit_on_texts(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIO 태그 사전 크기 : 8\n",
      "단어 사전 크기 : 13834\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전 및 태그 사전 크기\n",
    "vocab_size = len(sent_tokenizer.word_index) + 1\n",
    "tag_size = len(tag_tokenizer.word_index) + 1\n",
    "print(\"BIO 태그 사전 크기 :\", tag_size)\n",
    "print(\"단어 사전 크기 :\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msent_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "print(sent_tokenizer.word_index[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13833\n"
     ]
    }
   ],
   "source": [
    "print(len(sent_tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 1, 'I': 2, 'B_OG': 3, 'B_PS': 4, 'B_DT': 5, 'B_LC': 6, 'B_TI': 7}\n"
     ]
    }
   ],
   "source": [
    "print(tag_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['한편',\n",
       " ',',\n",
       " 'AFC',\n",
       " '챔피언스',\n",
       " '리그',\n",
       " 'E',\n",
       " '조',\n",
       " '에',\n",
       " '속하',\n",
       " 'ㄴ',\n",
       " '포항',\n",
       " '역시',\n",
       " '대회',\n",
       " '8강',\n",
       " '진출',\n",
       " '이',\n",
       " '불투명',\n",
       " '하',\n",
       " '다',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B_OG',\n",
       " 'I',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[183, 11, 4276, 884, 162, 931, 402, 10, 2608, 7, 1516, 608, 145, 1361, 414, 4, 6347, 2, 8, 3] 20\n",
      "[1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 20\n"
     ]
    }
   ],
   "source": [
    "# 학습용 단어 시퀀스 생성\n",
    "x_train = sent_tokenizer.texts_to_sequences(sentences)\n",
    "y_train = tag_tokenizer.texts_to_sequences(tags)\n",
    "print(x_train[0], len(x_train[0]))  # x_train[0] is the word_indexes of sentences[0] above.\n",
    "print(y_train[0], len(y_train[0]))  # y_train[0] is the word_indexes of tags[0] above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index to word / index to NER 정의\n",
    "index_to_word = sent_tokenizer.index_word # 시퀀스 인덱스를 단어로 변환하기 위해 사용\n",
    "index_to_ner = tag_tokenizer.index_word # 시퀀스 인덱스를 NER로 변환하기 위해 사용\n",
    "index_to_ner[0] = 'PAD'\n",
    "\n",
    "# Named Entity Recognition (NER) is a natural language processing task that involves \n",
    "# identifying and classifying named entities \n",
    "# (such as person names, locations, organizations, dates, etc.) in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(index_to_word)  # only opposite order.   word_index <---> index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'O', 2: 'I', 3: 'B_OG', 4: 'B_PS', 5: 'B_DT', 6: 'B_LC', 7: 'B_TI', 0: 'PAD'}\n"
     ]
    }
   ],
   "source": [
    "print(index_to_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  183    11  4276 ...     0     0     0]\n",
      " [ 1910    42    73 ...     0     0     0]\n",
      " [ 6352    11  6353 ...    14     8     3]\n",
      " ...\n",
      " [  387  1820    13 ...     0     0     0]\n",
      " [  531    16 13829 ...     0     0     0]\n",
      " [13831   398   451 ...     3     0     0]]\n",
      "---------see elements of x_train---------\n",
      "[ 183   11 4276  884  162  931  402   10 2608    7 1516  608  145 1361\n",
      "  414    4 6347    2    8    3    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "[1910   42   73   44  231   24 2609  430   64  206 4277   84  513   14\n",
      "   62   67  748 6348    5 1517   55   42   46   10 3252    6  932   23\n",
      "    6  275    4   18    8    3    0    0    0    0    0    0]\n",
      "-----------------------------------------\n",
      "[[1 1 1 ... 0 0 0]\n",
      " [5 2 2 ... 0 0 0]\n",
      " [2 1 4 ... 1 1 1]\n",
      " ...\n",
      " [3 2 1 ... 0 0 0]\n",
      " [3 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 시퀀스 패딩 처리\n",
    "max_len = 40\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, padding='post', maxlen=max_len)\n",
    "y_train = preprocessing.sequence.pad_sequences(y_train, padding='post', maxlen=max_len)\n",
    "print(x_train)\n",
    "print('---------see elements of x_train---------')\n",
    "print(x_train[0]) # padding at the end with 0\n",
    "print(x_train[1])\n",
    "print('-----------------------------------------')\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 샘플 시퀀스 형상 :  (2844, 40)\n",
      "학습 샘플 레이블 형상 :  (2844, 40, 8)\n",
      "테스트 샘플 시퀀스 형상 :  (711, 40)\n",
      "테스트 샘플 레이블 형상 :  (711, 40, 8)\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터와 테스트 데이터를 8:2 비율로 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=.2, random_state=0)\n",
    "\n",
    "# 출력 데이터를 원-핫 인코딩\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=tag_size)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=tag_size)\n",
    "\n",
    "print(\"학습 샘플 시퀀스 형상 : \", x_train.shape)\n",
    "print(\"학습 샘플 레이블 형상 : \", y_train.shape)\n",
    "print(\"테스트 샘플 시퀀스 형상 : \", x_test.shape)\n",
    "print(\"테스트 샘플 레이블 형상 : \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 430, 4417,   16,  416,   17,    9, 6582,   13, 6583,    2,   14,\n",
       "          8,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0] # one-hot encoded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 26s 574ms/step - loss: 0.6940 - accuracy: 0.8384\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 12s 527ms/step - loss: 0.3153 - accuracy: 0.9000\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 14s 587ms/step - loss: 0.2140 - accuracy: 0.9275\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 13s 585ms/step - loss: 0.1520 - accuracy: 0.9503\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1077 - accuracy: 0.9676\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 13s 564ms/step - loss: 0.0759 - accuracy: 0.9774\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 14s 595ms/step - loss: 0.0575 - accuracy: 0.9821\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 13s 579ms/step - loss: 0.0463 - accuracy: 0.9856\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 13s 539ms/step - loss: 0.0378 - accuracy: 0.9882\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 14s 604ms/step - loss: 0.0317 - accuracy: 0.9898\n",
      "23/23 [==============================] - 3s 57ms/step - loss: 0.2784 - accuracy: 0.9359\n",
      "평가 결과 :  0.9359340071678162\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의(Bi-LSTM)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=30, input_length=max_len, mask_zero=True))\n",
    "\n",
    "model.add(Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25)))\n",
    "# in the case of LSTM(200), it means that the LSTM layer consists of 200 memory units or hidden units, \n",
    "# and each unit performs computations in parallel at each time step. Each memory unit captures and processes information over time, \n",
    "# and the outputs of these units are combined and passed on to the next layer or time step.\n",
    "\n",
    "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
    "#TimeDistributed layer: This layer applies a Dense layer to each time step's input separately. \n",
    "#It allows the model to learn and apply a prediction for each token in the input sequence.\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.01), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=10)\n",
    "print(\"평가 결과 : \", model.evaluate(x_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀀스를 NER 태그로 변환\n",
    "def sequences_to_tag(sequences):\n",
    "    result = []\n",
    "    for sequence in sequences:\n",
    "        temp = []\n",
    "        for pred in sequence:\n",
    "            pred_index = np.argmax(pred)\n",
    "            temp.append(index_to_ner[pred_index].replace(\"PAD\", \"O\"))\n",
    "        result.append(temp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  69    7 1407   42 7891  581  206 2409   19 1560 4932    2   15  604\n",
      "   66 2047   25   28 7892  344   13  271    5  358    4  121    8    3\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 3s 70ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python38\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_DT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\python38\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_PS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\python38\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_OG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\python38\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_TI seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\python38\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_LC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.58      0.57      0.58       657\n",
      "         _DT       0.93      0.88      0.91       335\n",
      "         _LC       0.69      0.59      0.64       312\n",
      "         _OG       0.70      0.56      0.62       481\n",
      "         _PS       0.73      0.48      0.58       374\n",
      "         _TI       0.86      0.82      0.84        66\n",
      "\n",
      "   micro avg       0.71      0.61      0.65      2225\n",
      "   macro avg       0.75      0.65      0.69      2225\n",
      "weighted avg       0.71      0.61      0.65      2225\n",
      "\n",
      "F1-score: 65.5%\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터셋의 NER 예측\n",
    "y_predicted = model.predict(x_test) # (711, 40) => model => (711, 40, 8)\n",
    "pred_tags = sequences_to_tag(y_predicted) # 예측된 NER\n",
    "test_tags = sequences_to_tag(y_test) # 실제 NER\n",
    "\n",
    "# F1 스코어 계산을 위해 사용\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "print(classification_report(test_tags, pred_tags))\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B_DT', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['B_DT', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(pred_tags[0])\n",
    "print(test_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새로운 유형의 시퀀스 :  [531, 307, 1476, 286, 1507, 6766, 1]\n",
      "새로운 유형의 패딩된 시퀀스 :  [[ 531  307 1476  286 1507 6766    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "# 새로운 유형의 문장 NER 예측\n",
    "word_to_index = sent_tokenizer.word_index\n",
    "new_sentence = '삼성전자 출시 스마트폰 오늘 애플 도전장 내밀다.'.split()\n",
    "new_x = []\n",
    "for w in new_sentence:\n",
    "    try:\n",
    "        new_x.append(word_to_index.get(w, 1))\n",
    "    except KeyError:\n",
    "        # 모르는 단어의 경우 OOV\n",
    "        new_x.append(word_to_index['OOV'])\n",
    "\n",
    "print(\"새로운 유형의 시퀀스 : \", new_x)\n",
    "new_padded_seqs = preprocessing.sequence.pad_sequences([new_x], padding=\"post\", value=0, maxlen=max_len)\n",
    "print(\"새로운 유형의 패딩된 시퀀스 : \", new_padded_seqs)\n",
    "print(len(new_padded_seqs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 73ms/step\n",
      "p :  [[[1.14090897e-08 4.71777275e-05 3.35414516e-05 9.99789417e-01\n",
      "   1.26415704e-04 4.02266380e-07 3.03250022e-06 7.17077953e-10]\n",
      "  [1.76694233e-08 9.99198020e-01 7.94239342e-04 5.22262144e-06\n",
      "   2.07837229e-06 1.81124449e-07 2.30003820e-07 2.63181144e-09]\n",
      "  [3.41471974e-07 9.99873281e-01 4.21222358e-05 3.34498072e-05\n",
      "   9.97941606e-06 1.58340263e-05 2.49230634e-05 4.99451396e-08]\n",
      "  [4.85833198e-07 2.72453268e-04 1.01488104e-05 2.05194403e-04\n",
      "   1.56557025e-08 9.99207437e-01 2.05381759e-04 9.89314867e-05]\n",
      "  [1.75834120e-05 2.93629663e-03 1.70796812e-02 9.77409363e-01\n",
      "   2.02865092e-04 1.23806632e-04 2.20340630e-03 2.70485743e-05]\n",
      "  [1.21113926e-03 5.00886142e-01 4.80825007e-01 5.79342153e-03\n",
      "   4.10142005e-04 2.25596596e-03 4.72472515e-03 3.89338634e-03]\n",
      "  [4.17524064e-03 5.28384626e-01 4.28981870e-01 8.61465838e-03\n",
      "   1.21535745e-03 3.55494837e-03 7.58039393e-03 1.74929388e-02]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]\n",
      "  [1.19524889e-01 1.48132503e-01 1.20998979e-01 1.21285006e-01\n",
      "   1.26297817e-01 1.20239891e-01 1.21872924e-01 1.21647984e-01]]]\n",
      "index of each maximum elements of p :  [[3 1 1 5 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1]]\n",
      "length of p : 40\n",
      "단어         예측된 NER\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# NER 예측\n",
    "p = model.predict(np.array([new_padded_seqs[0]]))\n",
    "print('p : ', p)\n",
    "p = np.argmax(p, axis=-1)\n",
    "print('index of each maximum elements of p : ', p)\n",
    "print('length of p :', len(p[0]))# 예측된 NER 인덱스값 추출\n",
    "# only 7 is real values, rest is padding!\n",
    "print(\"{:10} {:5}\".format(\"단어\", \"예측된 NER\"))\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성전자 3\n",
      "삼성전자       B_OG \n",
      "--------------------\n",
      "출시 1\n",
      "출시         O    \n",
      "--------------------\n",
      "스마트폰 1\n",
      "스마트폰       O    \n",
      "--------------------\n",
      "오늘 5\n",
      "오늘         B_DT \n",
      "--------------------\n",
      "애플 3\n",
      "애플         B_OG \n",
      "--------------------\n",
      "도전장 1\n",
      "도전장        O    \n",
      "--------------------\n",
      "내밀다. 1\n",
      "내밀다.       O    \n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for w, pred in zip(new_sentence, p[0]):\n",
    "    print(w, pred)\n",
    "    print(\"{:10} {:5}\".format(w, index_to_ner[pred]))\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
